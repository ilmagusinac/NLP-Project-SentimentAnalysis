# -*- coding: utf-8 -*-
"""SENTIMENT ANALYSIS IMPLEMENTATION USING NLP ON GOOGLE PLAY STORE REVIEWS AND REPLIES - NLP project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WMFM_rO8kt6Xlpt6VXlg5UQy_xS7Wl7E

#                    **Google Play Reviews Sentiment Analysis**

                                             Project Type: Sentiment Analysis



> ***University:*** *International Burch University*


> ***Course:*** *IT 335 - Introduction to Natural Languages Processing*

> ***Students:*** *Sara Nalo & Ilma Gusinac*

                                                    January, 2024.

> **Introduction**

*Natural language processing (NLP)* is a branch of artificial intelligence, computer science, linguistics, and information engineering that studies how computers and human (natural) languages interact. It focuses on teaching computers how to process and analyze vast amounts of natural language data.

* Offering data analysis to developers to determine what customers are most likely to download and which category has the most downloads is essential to the app development process.
* With the implementation of machine learning and sentiment analysis using NLP, this project seeks to analyzing customer preferences and recommending to developers the ideal app to create.
* Using the dataset that is gathered from Kaggle, we will assist the users by classifying reviews and comments of the specific as neutral, negative, or favorable as well as give an overview forecast for the app category that will see the most downloads in the upcoming years.

> **Connecting Colab with Google Drive**


Interacting with Google Drive to mount the dataset content that is needed for the project. The dataset is located in the Google Drive section of the project.
"""

from google.colab import drive
drive.mount('/content/drive')

"""> **Dataset information**

The data contains over 12000 reviews of different app store applications by real users. As well as the rating that was given by them so it can be classified into positive or negative reviews.

- Twelve columns that make up the dataset are: 'reviewId' 'userName', 'userImage', 'content', 'score', 'thumbsUpCount' 'reviewCreatedVersion', 'at', 'replyContent', 'repliedAt', 'sortOrder' and 'appId'

# **1. Data reading**

* Initial step in reading and processing data from a CSV file called *reviews.csv.*
* Having an overview on the type of the columns that are present in the dataset
"""

# Import needed libraries
import pandas as pd
import os

# Read the files and assign them to apps, reviews values
reviews = pd.read_csv(r"/content/drive/MyDrive/nlp_project/reviews.csv")

# Print all unique values in 'score' column
print(reviews['score'].unique()) #scores are grouped together, so that 1 means that the user is not satisfied and 5 that it is satisfied

# Group reviews by score results
data = reviews.groupby(['score']).count().content.to_numpy()
data.sort()

# Print all column names in reviews dataset
print("*****************")
reviews.columns.values

"""Concise summary of the reviews DataFrame, including information about the index dtype and column dtypes, non-null values, and memory usage. This summary provides an overview of the DataFrame's structure and the data types of its columns."""

# Print info and first 5 rows of dataset
print(reviews.info())
reviews.head()

"""The column ***'content'*** is set to be the most important feature for this project, based on that we will get bit more in details about the sentiment analysis."""

# Print column 'content'
print(reviews['content'])

"""Based on the ***'replyContent'***, there will be a more comprehensive understanding about the analysis with the information on the content, the given score and reply from the developer."""

# Print column 'replyContent'
print(reviews['replyContent'])

"""To be able to fully understand everything that is going on with our dataset, it is needed to drop/delete unnecessary columns from the set, which means that the cleaning of the data begins with the droping columns set, since it will not be used for further prediction.

"""

# Delete/Drop columns reviewId, userName, userImage, thumbsUpCount, reviewCreatedVersion, at, repliedAt, appId and sortOrder since it will not be used for prediction

reviews.drop( 'reviewId', axis=1, inplace=True)
reviews.drop( 'userName', axis=1, inplace=True)
reviews.drop( 'userImage', axis=1, inplace=True)
reviews.drop( 'thumbsUpCount', axis=1, inplace=True)
reviews.drop( 'reviewCreatedVersion', axis=1, inplace=True)
reviews.drop( 'at', axis=1, inplace=True)
reviews.drop( 'repliedAt', axis=1, inplace=True)
reviews.drop( 'sortOrder', axis=1, inplace=True)
reviews.drop( 'appId', axis=1, inplace=True)
reviews.head() # printing first 5 rows from the cleaned dataset, with only 'content', 'score' and 'replyContent' columns

"""The ***'score'*** is set to be from [1-5], which gives freedom to users to give brief score for the app.
- On the graph, number *'3'*, is presented as the most repeted score in the distribution, which could actually be assumed as it represents the golden middle
"""

# Plot a graf to represent the distribution

plot = reviews.groupby("score").size().plot(kind="bar", title="Number of records by score")

"""Null values could cause problem when performing the prediction and accuracy for the given set.
- Only ***'replyContent'*** has null values, just because there could be a chance that there isn't a reply to some content, and it is automatically set to null.
"""

# Checking for null values since they may cause problem in accuracy and later in prediction

reviews.isnull().sum() #sum of all nul values in reviews column

"""# **2. Cleaning the dataset and preprocessing of the data - users reviews**

Cleaning and preprocessing the dataset ensures that the data is in optimal condition for analysis and model training. This section covers the steps involved in cleaning the dataset, filling in any missing values, and carrying out necessary preprocessing that is specific to the kind of user reviews.

- ***dropna()*** function removes rows where the *'replyContent'* column has null values.
- The subset parameter specifies the columns to consider, and inplace=True ensures that the changes are made directly to the original DataFrame (reviews), rather than creating a new one.
"""

# Delete all null values with function dropna(), we know they are in columns 'replyContent'

reviews.dropna(subset=['replyContent'], inplace=True)

reviews.isnull().sum() #here we ensure that there are no more null values that would not disrupt our prediction

"""Function, named ***cleanText()***, is designed to clean text data by removing various elements such as punctuation (except spaces), numbers, emojis, and converting the text to lowercase. This function can be applied to each text entry in a dataset to preprocess the textual data before analysis or model training."""

# Function to clean text data with regex from punctations, hyperlinks, non-alphabet characters and anything irrelevant

import re

def cleanText(text):
    text = re.sub("[^a-zA-Z']", '', text) # remove all punctuation except '
    text = re.sub(r'\d+', '', text)  # remove numbers
    text = text.encode('ascii', 'ignore').decode('ascii')  # remove emojis
    text = text.lower() # convert to lowercase letters
    return text

"""***tokenizationAndCleaning()*** function is a comprehensive preprocessing step that combines tokenization, cleaning, lemmatization, and stop word removal to prepare text data."""

# Function to tokenize text, clean it using the previous function with regex and from stop words.
# Using the spacy library allows us to do tokenization, part-of-speech tagging and lemmatization in this function.
# !pip install spacy

import nltk
import spacy
nltk.download('punkt')
nltk.download('stopwords')

nlp = spacy.load("en_core_web_sm") # Specific identifier for the English language model with a small size. spaCy

stopwords = nltk.corpus.stopwords.words('english')
print(stopwords) # All english stopwords

def tokenizationAndCleaning(text):
    # Tokenize using spaCy
    doc = nlp(text)

    # Clean and remove stop words
    cleanWordTokens = [cleanText(token.lemma_) for token in doc if token.is_alpha] # Retrieves lemmatized tokens (base forms of words) while excluding non-alphabetic tokens

    # Remove stop words
    cleanWordTokensFromStopwords = [w for w in cleanWordTokens if w.lower() not in stopwords]

    return " ".join(cleanWordTokensFromStopwords)

"""Checking the correctness of the function, by testing the first sentence in the ***'content'*** column."""

# Testing out the functions on one of the reviews

sentence = reviews['content'][0]
print(sentence)

print("****************")

tokenizationAndCleaning(sentence)

"""Applying the ***tokenizationAndCleaning*** function to each review in the *'content'* column of our DataFrame and storing the processed results in a new column named ***'processedContent'***. This is a common and necessary step in preparing text data for nlp."""

# Applying functions for all reviews in our dataset

reviews['processedContent'] = [tokenizationAndCleaning(review) for review in reviews['content']]

# Print out our data set with the new column

reviews

"""Here we demonstrate the use of various stemmers and lemmatizers (Porter, Lancaster, WordNet, and spaCy) to a sample sentence. It prints the original sentence along with the results of each stemmer and lemmatizer, showcasing how these techniques modify or transform the words in the given sentence.

- In this specific case, the stemmers (Porter and Lancaster) don't have a noticeable impact on the sentence because they are designed to truncate words, and the sentence already contains a lot of short words.
- The lemmatizers (WordNet and spaCy) produce similar results, reducing words to their base forms.
"""

# Applying Stemmer and Lemmatizer to determine which works best for our dataset
# Library imports
"""import spacy
nltk.download('wordnet')
nltk.download('punkt')

from nltk.corpus import wordnet

# Load spaCy English language model
nlp = spacy.load("en_core_web_sm")

sentence = reviews['processedContent'][0] #  Sample sentence from 'processedContent' column

porterStemmer = nltk.PorterStemmer()
lancasterStemmer = nltk.LancasterStemmer()

print("Sentence: ", sentence)
print("------------------")
print("Porter stemmer: ", porterStemmer.stem(sentence))
print("------------------")
print("Lancaster stemmer: ", lancasterStemmer.stem(sentence))

print("------------------")

# wordNetLemmatizer = nltk.WordNetLemmatizer()
print("WordNet lemmatizer: ", wordNetLemmatizer.lemmatize(sentence))

print("------------------")
"""

"""Next, we experimented with different stemming and lemmatization techniques on the *'processedContent'* column of your DataFrame to observe their impact on the text. The last line of code indicates that we've chosen to use spaCy for lemmatization based on the best accuracy."""

# All results are the same based on the first sentence of the column, so we checked which accuracy is best

# reviews['processedContent'] = [porterStemmer.stem(review) for review in reviews['processedContent']]
# reviews['processedContent'] = [lancasterStemmer.stem(review) for review in reviews['processedContent']]
# reviews['processedContent'] = [wordNetLemmatizer.lemmatize(review) for review in reviews['processedContent']]

# Check our dataset

# reviews

"""These next parts of the code are used to find and visualize the most frequent words in reviews that gave an app a score of 1/2/3/4/5. This is important for us to visualise and understand common complaints/issues/satisfaction/neutral thoughts reported by dissatisfied users.

*Most frequent words in reviews that gave an app a score of 1*

- Common complaints or issues reported by very dissatisfied users.

- The words in this list are individual tokens extracted from the reviews. They include a mix of nouns, verbs, adjectives, and other parts of speech. Commonly occurring words like "app", "update", "use", and "calendar" suggest key areas of user dissatisfaction or issues with the app.

- Here some words that are mostly used like "app" does not give us the emotion of a typical very dissatisfied user because most sentences begin or have the word "app" in them, because of the structure of it.
"""

# Get data about most used words when the app score is 1

allWordsScoreOne = []

for review in reviews[reviews['score'] == 1]['processedContent']:
  for word in nltk.word_tokenize(review):
    allWordsScoreOne.append(word)

print(allWordsScoreOne)

nltk.FreqDist(allWordsScoreOne).plot(10)

"""*Most frequent words in reviews that gave an app a score of 2*

- Common complaints or issues reported by dissatisfied users.

- The words in this list are individual tokens extracted from the reviews. They include a mix of nouns, verbs, adjectives, and other parts of speech. Commonly occurring words like "get", "task", "use", and "time" suggest key areas of user dissatisfaction or issues with the app.
- Here some words that are mostly used like "app" does not give us the emotion of a typical dissatisfied user because most sentences begin or have the word "app" in them, because of the structure of it.
"""

# Get data about most used words when the app score is 2

allWordsScoreTwo = []

for review in reviews[reviews['score'] == 2]['processedContent']:
  for word in nltk.word_tokenize(review):
    allWordsScoreTwo.append(word)

print(allWordsScoreTwo)

nltk.FreqDist(allWordsScoreTwo).plot(10)

"""*Most frequent words in reviews that gave an app a score of 3*

- Common neutral complaints or issues reported by neutral users.

- The words in this list are individual tokens extracted from the reviews. They include a mix of nouns, verbs, adjectives, and other parts of speech. Commonly occurring words like "good", "work", "get", and "like" suggest key areas of users neutral response.
- Here some words that are mostly used like "app" does not give us the emotion of a typical neutral user because most sentences begin or have the word "app" in them, because of the structure of it.
"""

# Get data about most used words when the app score is 3

allWordsScoreThree = []

for review in reviews[reviews['score'] == 3]['processedContent']:
  for word in nltk.word_tokenize(review):
    allWordsScoreThree.append(word)

print(allWordsScoreThree)

nltk.FreqDist(allWordsScoreThree).plot(10)

"""*Most frequent words in reviews that gave an app a score of 4*

- Common pleased replies reported by satisfied users.

- The words in this list are individual tokens extracted from the reviews. They include a mix of nouns, verbs, adjectives, and other parts of speech. Commonly occurring words like "like", "would", "use", and "great" suggest key areas of users satisfied response.

- Here some words that are mostly used like "app" does not give us the emotion of a typical neutral user because most sentences begin or have the word "app" in them, because of the structure of it.
"""

# Get data about most used words when the app score is 4

allWordsScoreFour = []

for review in reviews[reviews['score'] == 4]['processedContent']:
  for word in nltk.word_tokenize(review):
    allWordsScoreFour.append(word)

print(allWordsScoreFour)

nltk.FreqDist(allWordsScoreFour).plot(10)

"""*Most frequent words in reviews that gave an app a score of 5*

- Common highly pleased replies reported by very satisfied users.

- The words in this list are individual tokens extracted from the reviews. They include a mix of nouns, verbs, adjectives, and other parts of speech. Commonly occurring words like "love", "good", "great", and "really" suggest key areas of users very satisfied response.

- Here some words that are mostly used like "app" does not give us the emotion of a typical neutral user because most sentences begin or have the word "app" in them, because of the structure of it.
"""

# Get data about most used words when the app score is 5

allWordsScoreFive = []

for review in reviews[reviews['score'] == 5]['processedContent']:
  for word in nltk.word_tokenize(review):
    allWordsScoreFive.append(word)

print(allWordsScoreFive)

nltk.FreqDist(allWordsScoreFive).plot(10)

"""Preparing the dataset for ML model.
- Extracting the 'processedContent' -> our feature in dataset (features) and 'score' -> our class label in dataset (label), column from reviews DataFrame and assigning it to the variable X and Y.

The print(X) and print(y) statements are just for understanding purposes.
"""

# Take the features - 'processedContent' column as X variable and class - 'score' column as Y variable

X = reviews['processedContent'] # this is our feature in dataset - features
y = reviews['score'] # this is our class label in dataset - label

print(X)
print(y)

"""*Transforming text features into numeric values to count occurance of
each word in whole dataset for each row*
- ***CountVectorizer()*** represented as a feature extraction technique in scikit-learn that is used to transforms the text data into a numerical format by counting the occurrence of each word (token) in the dataset.

- ***fit_transform()*** method is applied to the 'processedContent' column (X). This method learns the vocabulary and returns a document-term matrix, where each row corresponds to a document (review), and each column corresponds to a unique word in the entire dataset. The values in the matrix represent the count of each word in the respective document.
"""

# Transforming text features into numeric values to count occurance of each word in whole dataset for each row

# Importing necessary libraries
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

tf = CountVectorizer() # Creating an instance of CountVectorizer
x_tf = tf.fit_transform(X) # Transforming the 'processedContent' column (X) into numeric values

"""Dividing dataset into train and test sets
- ***train_test_split()*** function is part of scikit-learn and is used for splitting arrays or matrices into random train and test subsets. In this case, it is applied to the transformed feature matrix (x_tf) and the target variable (y).
"""

# Dividing dataset into train and test sets

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x_tf, y, test_size = 0.2, stratify = y)

"""# **3. Trainig a model for prediction and evaluating the model - users reviews**

>  Introduction


With the dataset refined and preprocessed, the focus shifts to constructing a robust model capable of predicting sentiment or scores from user reviews. This section dives into the training phase, where algorithms learn patterns from the prepared data. Additionally, we explore the evaluation of the model, assessing its performance metrics and ensuring its effectiveness in handling real-world user sentiments.

- ***RandomForestClassifier***: type of ensemble learning method that constructs a multitude of decision trees during training and outputs the mode of the classes as the prediction for classification tasks.
- Model is trained using the training sets *(X_train and y_train)*. This process involves the model learning the patterns and relationships within the provided data to make predictions later.
"""

# Training model with train sets

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier() # Instantiate the RandomForestClassifier
model.fit(X_train,y_train) # Fit the model using the training sets

"""> Evaluate the accuracy of the model with test datasets

Interpreting the accuracy score:
- An accuracy of 1.0 (100%) would mean that all instances in the test dataset were correctly classified.
- An accuracy of 0.0 (0%) would indicate that none of the instances were classified correctly.

Score higher than 85% is presented as the best possible accuracy.
"""

# Evaluate the accuracy of the model with test datasets 85>

model.score(X_test, y_test)

"""> Confusion matrix

Using confusion matrix: visualize the predicted values and assess how well the model is performing for each class (in this case, each score: 1, 2, 3, 4, 5).

Each confusion matrix consists of four values:
- **True Positive** (*TP*): Instances correctly predicted as the positive class.
- **True Negative** (*TN*): Instances correctly predicted as the negative class.
- **False Positive** (*FP*): Instances incorrectly predicted as the positive class.
- **False Negative** (*FN*): Instances incorrectly predicted as the negative class.
"""

# With Confusion Matrix visualize the predicted values, which are predicted correct which not

from sklearn.metrics import multilabel_confusion_matrix

y_pred = model.predict(X_test) # Predicting values for the test dataset
cf_matrix = multilabel_confusion_matrix(y_test, y_pred, labels=[1,2,3,4,5]) # Creating a multilabel confusion matrix
print(cf_matrix)

"""Confusion matrix for score 1 indicates that above 1200 predictions of the model were ***True Negative***, which means the model correctly predicted the negative class. In simpler terms, the model correctly said “No” when it was indeed “No”. Between 200 and 400 predictions were ***True Positives***, which indicates that the model correctly predicted the positive class, meaning that the model correctly said “Yes” when it was indeed “Yes”. Both ***False Negatives*** and ***False Positives*** were below 200, and both of these mean that the model incorrectly predicted negative class when the true class was positive, and the model incorrectly predicted positive class when the true class was negative, respectively.

"""

import seaborn as sns
import numpy as np

#Confusion Matrix for first class - Score 1

labels = ['True Negative','False Positive','False Negative','True Positive']
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cf_matrix[0], annot=labels, fmt='', cmap='Blues')

"""Confusion matrix for score 2 indicates that above 1200 predictions of the model were ***True Negative***, which means the model correctly predicted the negative class. In simpler terms, the model correctly said “No” when it was indeed “No”. Between 200 and 400 predictions were ***True Positives***, which indicates that the model correctly predicted the positive class, meaning that the model correctly said “Yes” when it was indeed “Yes”. Both ***False Negatives*** and ***False Positives*** were below 200, and both of these mean that the model incorrectly predicted negative class when the true class was positive, and the model incorrectly predicted positive class when the true class was negative, respectively."""

# Confusion Matrix for second class - Score 2

labels = ['True Negative','False Positive','False Negative','True Positive']
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cf_matrix[1], annot=labels, fmt='', cmap='Reds')

"""Confusion matrix for score 3 indicates that above 700 predictions of the model were ***True Negative***, which means the model correctly predicted the negative class. In simpler terms, the model correctly said “No” when it was indeed “No”. Between 500 and 600 predictions were ***True Positives***, which indicates that the model correctly predicted the positive class, meaning that the model correctly said “Yes” when it was indeed “Yes”. Between 200 and 300 predictions were ***False Positives***, which means that the model incorrectly predicted positive class when the true class was negative. ***False Negative*** predictions were below 100, and this tells us that the model incorrectly predicted negative class when the true class was positive.

"""

# Confusion Matrix for third class - Score 3

labels = ['True Negative','False Positive','False Negative','True Positive']
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cf_matrix[2], annot=labels, fmt='', cmap='Greens')

"""Confusion matrix for score 4 indicates that above 1200 predictions of the model were ***True Negative***, which means the model correctly predicted the negative class. In simpler terms, the model correctly said “No” when it was indeed “No”. Both ***True Positives*** and ***False Negatives*** were between 400 and 600, which indicates that the model correctly predicted the positive class, meaning that the model correctly said “Yes” when it was indeed “Yes”, and the model incorrectly predicted a negative class when the true class was positive, respectively. ***False Positive*** predictions were below 200, and this tells us that the model incorrectly predicted positive class when the true class was negative."""

# Confusion Matrix for third class - Score 4

labels = ['True Negative','False Positive','False Negative','True Positive']
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cf_matrix[3], annot=labels, fmt='', cmap='Oranges')

"""Confusion matrix for score 5 indicates that above 1400 predictions of the model were ***True Negative***, which means the model correctly predicted the negative class. In simpler terms, the model correctly said “No” when it was indeed “No”. ***True Positives***, ***False Positives*** and ***False Negatives*** were all below 200, where True Positive means that the model correctly predicted the positive class, False Positive that the model incorrectly predicted positive class when the true class was negative, and False Negative that the model incorrectly predicted a negative class when the true class was positive.

"""

# Confusion Matrix for third class - Score 5

labels = ['True Negative','False Positive','False Negative','True Positive']
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cf_matrix[4], annot=labels, fmt='', cmap='Purples')

"""# **4. Cleaning the dataset and preprocessing of the data - replies on users reviews**

> Introduction

Cleaning the dataset refers to the process of identifying and handling any inconsistencies, errors, or irrelevant information in the raw data. Preprocessing involves transforming the raw data into a format that is suitable for analysis or machine learning algorithms.

- ***dropna()*** function removes rows where the 'replyContent' column has null values.
- The subset parameter specifies the columns to consider, and inplace=True ensures that the changes are made directly to the original DataFrame (reviews), rather than creating a new one.
"""

# Delete all null values with function dropna(), we know they are in columns 'replyContent'

reviews.dropna(subset=['replyContent'], inplace=True)

reviews.isnull().sum()

"""Function, named ***cleanText()***, is designed to clean text data by removing various elements such as punctuation (except spaces), numbers, emojis, and converting the text to lowercase. This function can be applied to each text entry in a dataset to preprocess the textual data before analysis or model training."""

# Function to clean text data with regex from punctations, hyperlinks, non-alphabet characters and anything irrelevant

import re

def cleanText(text):
    text = re.sub("[^a-zA-Z']", '', text) # remove all punctuation except spaces
    text = re.sub(r'\d+', '', text)  # remove numbers
    text = text.encode('ascii', 'ignore').decode('ascii')  # remove emojis
    text = text.lower() # convert to lowercase letters
    return text

"""***tokenizationAndCleaning()*** function is a comprehensive preprocessing step that combines tokenization, cleaning, lemmatization, and stop word removal to prepare text data."""

# Function to tokenize text, clean it using the previous function with regex and from stop words

import nltk
import spacy
nltk.download('punkt')
nltk.download('stopwords')

nlp = spacy.load("en_core_web_sm")

stopwords = nltk.corpus.stopwords.words('english')
# print(stopwords)

def tokenizationAndCleaning(text):
    # Tokenize using spaCy
    doc = nlp(text)

    # Clean and remove stop words
    cleanWordTokens = [cleanText(token.lemma_) for token in doc if token.is_alpha]

    # Remove stop words
    cleanWordTokensFromStopwords = [w for w in cleanWordTokens if w.lower() not in stopwords]

    return " ".join(cleanWordTokensFromStopwords)

"""Checking the correctness of the function, by testing the first sentence in the *'replyContent'* column."""

# Testing out the functions on one of the reviews

sentence = reviews['replyContent'][0]

print(sentence)
print("****************")

tokenizationAndCleaning(sentence)

"""Applying the *tokenizationAndCleaning* function to each review in the '*replyContent*' column of our DataFrame and storing the processed results in a new column named '*processedReplies*'. This is a common and necessary step in preparing text data for nlp."""

# Applying functions for all repiles in our dataset

reviews['processedReplies'] = [tokenizationAndCleaning(review) for review in reviews['replyContent']]

# Print out our data set with the new column

reviews

"""These next parts of the code are used to find and visualize the most frequent words in reviews that gave an app a score of 1/2/3/4/5 in the *'processedReplies'* . This is important for us to visualise and understand common complaints/issues/satisfaction/neutral thoughts reported by replies from the developers or other users that reply.

*Most frequent words in replies on user review that gave an app a score of 1*

The words in this list are individual tokens extracted from the replies on user reviews. They include a mix of nouns, verbs, adjectives, and other parts of speech. Commonly occurring words like "help", "hi", "sorry", and "issue" suggest key areas of reply types for the.

Here some words that are mostly used like "app" do not give us the emotion of a typical reply because most sentences begin or have the word "app" in them, because of the structure of it.
"""

# Get data about most used words when the app score is 1

allWordsScoreOne = []

for review in reviews[reviews['score'] == 1]['processedReplies']:
  for word in nltk.word_tokenize(review):
    allWordsScoreOne.append(word)

nltk.FreqDist(allWordsScoreOne).plot(10)

"""This segment employs nlp technique to identify and visualize the most frequent words associated with user reviews that have a score of one.

Using the N*atural Language Toolkit's* *FreqDist* function, it generates a frequency distribution of words in the specified reviews.
- The 10 most common words and their frequencies are then extracted.
- The code calculates the average percentage of each of these top words in the dataset.
- Finally, it visualizes this information through a bar chart, providing a clear representation of the relative prevalence of key words in reviews with a score of one.







"""

# Most common words for the score one
import matplotlib.pyplot as plt

fdist = nltk.FreqDist(allWordsScoreOne)

mostCommonWordsOne = fdist.most_common(10)
print(mostCommonWordsOne)

totalWords = len(allWordsScoreOne)
averagePercentages = {word: (count / totalWords) * 100 for word, count in mostCommonWordsOne}

plt.figure()
plt.bar(averagePercentages.keys(), averagePercentages.values())
plt.xlabel('Word')
plt.ylabel('Average Percentage')
plt.show()

"""*Most frequent words in replies on user review that gave an app a score of 2*

The words in this list are individual tokens extracted from the replies on user reviews. They include a mix of nouns, verbs, adjectives, and other parts of speech. Commonly occurring words like "feedback", "thank", "please", and "version" suggest key areas of reply types for the user review.

Here some words that are mostly used like "hi" do not give us the emotion of a typical reply because most sentences begin or have the word "hi" in them, because of the structure of it.
"""

# Get data about most used words when the app score is 2

allWordsScoreTwo = []

for review in reviews[reviews['score'] == 2]['processedReplies']:
  for word in nltk.word_tokenize(review):
    allWordsScoreTwo.append(word)


nltk.FreqDist(allWordsScoreTwo).plot(10)

# Most common words for score two

fdist = nltk.FreqDist(allWordsScoreTwo)

mostCommonWordsTwo = fdist.most_common(10)
print(mostCommonWordsTwo)

totalWords = len(allWordsScoreTwo)
averagePercentages = {word: (count / totalWords) * 100 for word, count in mostCommonWordsTwo}
print("Average Percentages:", averagePercentages)

# plot the average percentages
plt.figure()
plt.bar(averagePercentages.keys(), averagePercentages.values())
plt.xlabel('Word')
plt.ylabel('Average Percentage')
plt.show()

"""*Most frequent words in replies on user review that gave an app a score of 3*

The words in this list are individual tokens extracted from the replies on user reviews. They include a mix of nouns, verbs, adjectives, and other parts of speech. Commonly occurring words like "thank", "please", "feedback", and "help" suggest key areas of reply types for the user review.

Here some words that are mostly used like "hi" do not give us the emotion of a typical reply because most sentences begin or have the word "app" in them, because of the structure of it.
"""

# Get data about most used words when the app score is 3

allWordsScoreThree = []

for review in reviews[reviews['score'] == 3]['processedReplies']:
  for word in nltk.word_tokenize(review):
    allWordsScoreThree.append(word)

nltk.FreqDist(allWordsScoreThree).plot(10)

# Most common words for score 3

fdist = nltk.FreqDist(allWordsScoreThree)

mostCommonWordsThree = fdist.most_common(10)
print(mostCommonWordsThree)

totalWords = len(allWordsScoreThree)
averagePercentages = {word: (count / totalWords) * 100 for word, count in mostCommonWordsThree}
print("Average Percentages:", averagePercentages)

# plot the average percentages
plt.figure()
plt.bar(averagePercentages.keys(), averagePercentages.values())
plt.xlabel('Word')
plt.ylabel('Average Percentage')
plt.show()

"""*Most frequent words in replies on user review that gave an app a score of 4*

The words in this list are individual tokens extracted from the replies on user reviews. They include a mix of nouns, verbs, adjectives, and other parts of speech. Commonly occurring words like "hi", "suggestion", "review", and "please" suggest key areas of reply types for the user review.

Here some words that are mostly used like "thank" do not give us the emotion of a typical reply because most sentences begin or have the word "app" in them, because of the structure of it.
"""

# Get data about most used words when the app score is 4

allWordsScoreFour = []

for review in reviews[reviews['score'] == 4]['processedReplies']:
  for word in nltk.word_tokenize(review):
    allWordsScoreFour.append(word)

nltk.FreqDist(allWordsScoreFour).plot(10)

# Most common words for score 4

fdist = nltk.FreqDist(allWordsScoreFour)

mostCommonWordsFour = fdist.most_common(10)
print(mostCommonWordsFour)

totalWords = len(allWordsScoreFour)
averagePercentages = {word: (count / totalWords) * 100 for word, count in mostCommonWordsFour}
print("Average Percentages:", averagePercentages)

# plot the average percentages
plt.figure()
plt.bar(averagePercentages.keys(), averagePercentages.values())
plt.xlabel('Word')
plt.ylabel('Average Percentage')
plt.show()

"""*Most frequent words in replies on user review that gave an app a score of 5*

The words in this list are individual tokens extracted from the replies on user reviews. They include a mix of nouns, verbs, adjectives, and other parts of speech. Commonly occurring words like "hi", "feedback", "review", and "version" suggest key areas of reply types for the user review.

Here some words that are mostly used like "thank" do not give us the emotion of a typical reply because most sentences begin or have the word "app" in them, because of the structure of it.
"""

# Get data about most used words when the app score is 5

allWordsScoreFive = []

for review in reviews[reviews['score'] == 5]['processedReplies']:
  for word in nltk.word_tokenize(review):
    allWordsScoreFive.append(word)

nltk.FreqDist(allWordsScoreFive).plot(10)

# Most common words for score 5

fdist = nltk.FreqDist(allWordsScoreFive)

mostCommonWordsFive = fdist.most_common(10)
print(mostCommonWordsFive)

totalWords = len(allWordsScoreFive)
averagePercentages = {word: (count / totalWords) * 100 for word, count in mostCommonWordsFive}
print("Average Percentages:", averagePercentages)

# plot the average percentages
plt.figure()
plt.bar(averagePercentages.keys(), averagePercentages.values())
plt.xlabel('Word')
plt.ylabel('Average Percentage')
plt.show()